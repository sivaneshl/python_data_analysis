import networkx as nx
import pandas as pd
import numpy as np
import pickle
from sklearn.preprocessing import MinMaxScaler
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

# Part 1 - Random Graph Identification
# For the first part of this assignment you will analyze randomly generated graphs and
# determine which algorithm created them.
P1_Graphs = pickle.load(open('../resources/A4_graphs','rb'))
# print(P1_Graphs)

# P1_Graphs is a list containing 5 networkx graphs.
# Each of these graphs were generated by one of three possible algorithms:
# Preferential Attachment ('PA')
# Small World with low probability of rewiring ('SW_L')
# Small World with high probability of rewiring ('SW_H')
# Anaylze each of the 5 graphs and determine which of the three algorithms generated the graph.
# The graph_identification function should return a list of length 5 where each element in the
# list is either 'PA', 'SW_L', or 'SW_H'.
def degree_distribution(G):
    degrees = dict(G.degree())
    degree_values = sorted(set(degrees.values()))
    histogram = [list(degrees.values()).count(i) / len(G.nodes()) for i in degree_values]
    return len(histogram)

def graph_identification():
    graph_created_by = []
    for G in P1_Graphs:
        clustering = nx.average_clustering(G)
        shortest_path = nx.average_shortest_path_length(G)
        degree_dist_len = degree_distribution(G)

        if degree_dist_len > 10:
            graph_created_by.append('PA')
        elif clustering < 0.1:
            graph_created_by.append('SW_H')
        else:
            graph_created_by.append('SW_L')

    return graph_created_by

# print(graph_identification())

# Part 2 - Company Emails
# For the second part of this assignment you will be workking with a company's email network
# where each node corresponds to a person at the company, and each edge indicates that at least
# one email has been sent between two people.
# The network also contains the node attributes Department and ManagementSalary.
# Department indicates the department in the company which the person belongs to, and ManagementSalary
# indicates whether that person is receiving a management position salary.
G = nx.read_gpickle('../resources/email_prediction.txt')
# print(nx.info(G))

# Part 2A - Salary Prediction
# Using network G, identify the people in the network with missing values for the node attribute
# ManagementSalary and predict whether or not these individuals are receiving a management position salary.
# To accomplish this, you will need to create a matrix of node features using networkx, train a sklearn
# classifier on nodes that have ManagementSalary data, and predict a probability of the node receiving a
# management salary for nodes where ManagementSalary is missing.
# Your predictions will need to be given as the probability that the corresponding employee is receiving a
# management position salary.
# The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).
# Your grade will be based on the AUC score computed for your classifier. A model which with an AUC of 0.88
# or higher will receive full points, and with an AUC of 0.82 or higher will pass (get 80% of the full points).
# Using your trained classifier, return a series of length 252 with the data being the probability of
# receiving management salary, and the index being the node id.
# Example:
#     1       1.0
#     2       0.0
#     5       0.8
#     8       1.0
#         ...
#     996     0.7
#     1000    0.5
#     1001    0.0
#     Length: 252, dtype: float64


def is_management(node):
    managementSalary = node[1]['ManagementSalary']
    if managementSalary == 0:
        return 0
    elif managementSalary == 1:
        return 1
    else:
        return None

def salary_predictions():
    df = pd.DataFrame(index=G.nodes())
    # df = pd.DataFrame(index=list(G.node.keys()))
    df['ManagementSalary'] = pd.Series(nx.get_node_attributes(G, 'ManagementSalary'))
    df['Department'] = pd.Series(nx.get_node_attributes(G, 'Department'))
    # df['ManagementSalary'] = pd.Series([G.node[node]['ManagementSalary'] for node in list(G.node.keys())])
    # df['Department'] = pd.Series([G.node[node]['Department'] for node in list(G.node.keys())])
    df['clustering'] = pd.Series(nx.clustering(G))
    df['degree'] = pd.Series(G.degree())
    df['degCent'] = pd.Series(nx.degree_centrality(G))
    df['closeCent'] = pd.Series(nx.closeness_centrality(G, normalized=True))
    df['btwnCent'] = pd.Series(nx.betweenness_centrality(G, normalized=True))
    df['pageRank'] = pd.Series(nx.pagerank(G))
    # df['is_management'] = pd.Series([is_management(node) for node in G.nodes(data=True)])
    # print(df.head(10))

    df_train = df[~pd.isnull(df['ManagementSalary'])]
    df_test = df[pd.isnull(df['ManagementSalary'])]
    # print(df_train.head())
    # print(df_test.head())

    features = ['clustering', 'degree', 'degCent', 'closeCent', 'btwnCent', 'pageRank']
    X_train = df_train[features]
    y_train = df_train['ManagementSalary']
    X_test = df_test[features]

    scalar = MinMaxScaler()
    X_train_scaled = scalar.fit_transform(X_train)
    X_test_scaled = scalar.fit_transform(X_test)

    # clf = SVC(C=10, kernel='rbf', gamma='auto', probability=True).fit(X_train_scaled, y_train)
    # clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, max_depth=10, random_state=0).fit(X_train_scaled, y_train)
    # clf = MLPClassifier(hidden_layer_sizes=[10, 5], alpha=5, random_state=0, solver='lbfgs', verbose=0).fit(X_train_scaled, y_train)
    clf = MLPClassifier(solver='lbfgs',
                        max_iter=10000,
                        activation='tanh',
                        alpha=0.1,
                        hidden_layer_sizes=[10, 10],
                        random_state=0).fit(X_train_scaled, y_train)
    test_proba = clf.predict_proba(X_test_scaled)[:, 1]
    return pd.Series(test_proba, X_test.index)

print(salary_predictions())

# Part 2B - New Connections Prediction
# For the last part of this assignment, you will predict future connections between
# employees of the network. The future connections information has been loaded into the
# variable future_connections. The index is a tuple indicating a pair of nodes that
# currently do not have a connection, and the Future Connection column indicates if an
# edge between those two nodes will exist in the future, where a value of 1.0 indicates a future connection.
future_connections = pd.read_csv('../resources/Future_Connections.csv', index_col=0, converters={0: eval})
# print(future_connections.head(10))

# Using network G and future_connections, identify the edges in future_connections with
# missing values and predict whether or not these edges will have a future connection.
# To accomplish this, you will need to create a matrix of features for the edges found in
# future_connections using networkx, train a sklearn classifier on those edges in
# future_connections that have Future Connection data, and predict a probability of the edge
# being a future connection for those edges in future_connections where Future Connection is missing.
# Your predictions will need to be given as the probability of the corresponding edge being a
# future connection.
# The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).
# Your grade will be based on the AUC score computed for your classifier. A model which with an
# AUC of 0.88 or higher will receive full points, and with an AUC of 0.82 or higher will pass
# (get 80% of the full points).
# Using your trained classifier, return a series of length 122112 with the data being the probability
# of the edge being a future connection, and the index being the edge as represented by a tuple of nodes.
# Example:
#     (107, 348)    0.35
#     (542, 751)    0.40
#     (20, 426)     0.55
#     (50, 989)     0.35
#               ...
#     (939, 940)    0.15
#     (555, 905)    0.35
#     (75, 101)     0.65
#     Length: 122112, dtype: float64
def new_connections_predictions():
    pref_attach = list(nx.preferential_attachment(G))
    df = pd.DataFrame(index=[(x[0], x[1]) for x in pref_attach])
    df['pref_attch'] = [x[2] for x in pref_attach]

    common_neigh = [(e[0], e[1], len(list(nx.common_neighbors(G, e[0], e[1])))) for e in nx.non_edges(G)]
    df1 = pd.DataFrame(index=[(x[0], x[1]) for x in common_neigh])
    df1['common_neigh'] = [x[2] for x in common_neigh]
    df = df.join(df1, how='outer')
    df['common_neigh'] = df['common_neigh'].fillna(value=0)
    del df1

    community_common_neigh = list(nx.cn_soundarajan_hopcroft(G, community='Department'))
    df1 = pd.DataFrame(index=[(x[0], x[1]) for x in community_common_neigh])
    df1['community_common_neigh'] = [x[2] for x in community_common_neigh]
    df = df.join(df1, how='outer')
    df['community_common_neigh'] = df['community_common_neigh'].fillna(value=0)
    del df1

    community_res_alloc = list(nx.ra_index_soundarajan_hopcroft(G, community='Department'))
    df1 = pd.DataFrame(index=[(x[0], x[1]) for x in community_res_alloc])
    df1['community_res_alloc'] = [x[2] for x in community_res_alloc]
    df = df.join(df1, how='outer')
    df['community_res_alloc'] = df['community_res_alloc'].fillna(value=0)
    del df1

    df['res_alloc'] = [x[2] for x in list(nx.resource_allocation_index(G))]
    df['jaccard_coeff'] = [x[2] for x in list(nx.jaccard_coefficient(G))]

    features = ['jaccard_coeff', 'res_alloc', 'pref_attch', 'common_neigh', 'community_common_neigh', 'community_res_alloc']

    df = future_connections.join(df, how='outer')
    df_train = df[~pd.isnull(df['Future Connection'])]
    df_test = df[pd.isnull(df['Future Connection'])]

    X_train = df_train[features]
    X_test = df_test[features]
    y_train = df_train['Future Connection']

    scalar = MinMaxScaler()
    X_train_scaled = scalar.fit_transform(X_train)
    X_test_scaled = scalar.fit_transform(X_test)

    clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, max_depth=10, random_state=0).fit(X_train_scaled, y_train)
    test_proba = clf.predict_proba(X_test_scaled)[:, 1]
    predictions = pd.Series(test_proba, X_test.index)
    # target = future_connections[pd.isnull(future_connections['Future Connection'])]
    # target['proba'] = [predictions[x] for x in target.index]
    return predictions
    # return target['proba']


print(new_connections_predictions())